{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "561c1a93-4098-4e4f-9332-dc79169fd2cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run this Python notebook using a classic compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c945bfd-59ad-4ec8-914e-f2185375cb05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schema, source volume, and target volume"
    }
   },
   "outputs": [],
   "source": [
    "schema_use = \"primary_clinical_data\"\n",
    "source_volume = \"/Volumes/nebraska_health_demo/synthea/synthea/output/fhir/\"\n",
    "target_volume = f\"/Volumes/nebraska_health_demo/primary_clinical_data/landing/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "672253d1-f4db-4f80-a477-ca30949131c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using Shell Commands to Interact with Volumes \n",
    "***\n",
    "\n",
    "One of the best parts about using Volumes to reference your cloud storage is that it provides a posix-style way of interacting with the storage account, abstracting away the conplexities of each cloud providers various APIs.  Additionally it allows the use of standard shells commands such as `ls`, `cp`, `mv`, `mkdir`, etc. as though it were local storage.  This makes interacting with files incredibly easy in Databricks.  \n",
    "\n",
    "When connected to Serverless Notebook compute or a Standard or Dedicated cluster (i.e. not a SQL warehouse), we're able to use the `%sh` magic command to directly execute shell commands.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e481b5c5-3872-4176-b358-2801aa6f1542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78648624-c0ba-4e96-a511-eca14e9f98e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "How many files?"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "%sh\n",
    "ls -1 /Volumes/fhir_workshop/synthea/synthetic_files_raw/output/fhir/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12edb9c4-3b96-4cd1-bc95-d2269dfc9ba0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "How Many Gigabytes Are We Loading?"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "%sh \n",
    "du -sh /Volumes/fhir_workshop/synthea/synthetic_files_raw/output/fhir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e07fb89a-8951-4a23-bf31-4edceda451f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "What about the synthea_55k Schema?"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# %sh\n",
    "# cd /Volumes/fhir_workshop/synthea_55k/synthetic_files_raw/output/fhir/;\n",
    "# ls -1 | wc -l;\n",
    "# du -sh;\n",
    "\n",
    "# 65,225 files\n",
    "# 299GB in total -- takes 12 minutes to get this result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a452f1e4-c725-4039-a443-4e35492e38b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Uncommenting and running the above shows that for the data used in the February 25, 2025 workshop, the number of files loaded was 65,225 JSONs, totaling 299GB.  Note that the shell commands take 12 minutes to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5a7cd0-c22d-482a-ac12-8c6208717477",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use Shell Commands Directly in the Notebook"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "%sh\n",
    "\n",
    "ls -alt /Volumes/fhir_workshop/synthea/synthetic_files_raw/output/fhir/ | head -n 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "930e96d0-797a-463d-af37-e9b3b761c993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can even use cat to inspect the contents of one of the files directly in the notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff263990-9031-4b81-8548-f40ba10246da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cat one of the JSON Files to View its Contents"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "%sh \n",
    "\n",
    "cat /Volumes/fhir_workshop/synthea/synthetic_files_raw/output/fhir/DBPriorAuthExample.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0ea9336-1ccb-4cf9-9a89-92a127a5f072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Shell context is preserved inside the code chunck, but not between code chunks.  This means we can seperate multiple shell commnds in the same code chunk with semi-colons, including the use of `cd` for the volume path!  \n",
    "\n",
    "Let's compare this FHIR bundle to another bundle but do it in a more programatic way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440644d8-6028-4cef-a838-904cba737357",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Shell Context is preserved inside the code chunk,"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "%sh \n",
    "\n",
    "cd /Volumes/fhir_workshop/synthea/synthetic_files_raw/output/fhir/;\n",
    "ls A*.json | head -n 1;\n",
    "ls A*.json | head -n 1 | awk '{print $1}' | xargs cat;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "311ac402-c44b-4778-a5c7-dec80ba484d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The motivation for this course is really based on the complexity of this FHIR bundles.  No two FHIR JSONs will ever have the exact same strcuture, and therefore the exact same schema.  While great for transmitting data between organizations, they are really not great for much anything else.  You need an easy way to parese these, and that's what the rest of this course is all about.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e76b732e-7725-410f-95ed-f7883a2b213b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define a function to Copy Files with a File Pattern\n",
    "***\n",
    "\n",
    "During the first iteration of this course, the source volume contained nearly 100K FHIR JSON bundles, which would have taken too long to copy over for everyone in the course.  Therefore an array of file patterns was used to copy over approximately 1,000 bundles from the source volume to the target volume.  The below function also makes use of standard Python libraries that interact with local file storage.  You may be used to using these already on your laptop or Linux VMs.  With Volumes you may use these same functions without the need to learn each cloud's storage APIs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000d738f-ce02-4e42-aff6-8f8dbda9f6bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Copy Files Function"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def copy_files(source_volume, target_volume, file_pattern):\n",
    "  # Check if the source and target volumes end with a slash\n",
    "  if not source_volume.endswith('/'):\n",
    "    source_volume += '/'\n",
    "\n",
    "  if not target_volume.endswith('/'):\n",
    "    target_volume += '/'\n",
    "\n",
    "  # Use glob to locate files based on the file pattern\n",
    "  if file_pattern is None:\n",
    "    file_pattern = '*'\n",
    "\n",
    "  files = glob.glob(os.path.join(source_volume, file_pattern))\n",
    "\n",
    "  # Copy each file to the destination directory\n",
    "  for file in files:\n",
    "    target_file = os.path.join(target_volume, os.path.basename(file))\n",
    "    if os.path.exists(target_file):\n",
    "        os.remove(target_file)\n",
    "    shutil.copy2(file, target_volume)\n",
    "\n",
    "  return f\"Copied {str(len(files))} files.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99bff775-b2af-48c5-bb2f-3702bc71acd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Uncomment the first line of the code chunk below to use file patterns based on the start of the FHIR bundle names and comment out the last line.  If the number of files in the source volume is less than 2K, then its fine to move everything with a file pattern of \"*\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c334db7-a46e-47ea-a1ff-eeb0d9bb3744",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Approximately First 1K Files With File Patterns"
    }
   },
   "outputs": [],
   "source": [
    " # file_patterns = [\"Aa*.json\", \"Ab*.json\", \"Ad*.json\", \"Af*.json\", \"Ag*.json\", \"Ah*.json\", \"Ai*.json\", \"Aj*.json\", \"Ak*.json\"]\n",
    "file_patterns = [\"*\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf7f947-5e27-4bcd-ab16-eb58ca817744",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Copy Files By Pattern"
    }
   },
   "outputs": [],
   "source": [
    "for file_pattern in file_patterns:\n",
    "    print(copy_files(source_volume, target_volume, file_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80aec17-9a77-4299-9b50-6b10f9a8c4e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optional - Delete the files in the target volume."
    }
   },
   "outputs": [],
   "source": [
    "# files_to_remove = dbutils.fs.ls(target_volume)\n",
    "# for file in files_to_remove:\n",
    "#     if file.name.startswith(\"Al\") and file.name.endswith(\".json\"):\n",
    "#         dbutils.fs.rm(file.path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7073063168670809,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Copy Files to Volume",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
