{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "520e6679-deec-42c1-89d1-abd5374a08be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run this SQL notebook in Serverless SQL Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78d394e-808b-459d-a1cc-5f7abbf42fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***\n",
    "# Streaming Bronze Tables Ingestion Using Autoloader\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd755e2-27f7-4606-a19a-2b1e318e62f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set the Schema to Use as a Declared Variable"
    }
   },
   "outputs": [],
   "source": [
    "DECLARE OR REPLACE VARIABLE schema_use STRING DEFAULT 'primary_clinical_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fabb7a-86d7-441e-8322-2a4bb979f0cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use Catalog and Schema Statement"
    }
   },
   "outputs": [],
   "source": [
    "USE IDENTIFIER(\"nebraska_health_demo.\" || schema_use);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd0e3371-dc4f-4e88-a1a0-f6544d4514f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Executing `CREATE OR REFRESH STREAMING TABLE` statements against a Serverless SQL warehouse automatically creates a Lakeflow (DLT) pipeline behind the scenes. These pipelines use Serverless compute. Databricks automatically takes care of capturing the state, checkpoints, cluster size, and pipeline creation to execute the streaming table.  Users only have to worry about writing SQL.  \n",
    "\n",
    "All streaming tables are \"batch\" unless the `CONTINUOUS` keyword is used, as in `CREATE OR REFRESH CONTINUOUS STREAMING TABLE`.  If a streaming table has already been defined then the `REFRESH` statement will process any data that is ready to be processed.  Once all data that is available has been processed, the stream is automatically stopped until the next `REFRESH` happens.  \n",
    "\n",
    "Normally we would never use the `DROP TABLE` statement.  For normal managed tables the `CREATE OR REPLACE` statement does the same thing as a truncate and reload while preserving the delta log, allowing for a back out strategy should the load need to be reversed.  This allows for better **DataOps**.  However since streaming tables are essentially Lakeflow (DLT) pipelines, a `FULL REFRESH` is essentially the same as a `DROP TABLE` combined with a `REFRESH`. \n",
    "\n",
    "Thefore, with streaming tables (or materialized views), we'll use the `FULL REFRESH` statement when we wish to completely reload the data over, and `DROP TABLE` followed by a `CREATE OR REFRESH STREAMING TABLE` statement when we need to be explict with our schema evolution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee320fef-a187-47f1-8f8f-6e399f69c9fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Refresh of the FHIR Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS fhir_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe3ad10-0388-4027-aa33-849709880298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We're going to create two bronze tables.  The first bronze will use Autoloader and therefore will need to be dynamic due to the differences in the volume paths here.  Recall that a Volume path always has a root of `/Volumes/<catalog>/<schema>/<volume_name>/`.  This means that pretty much any code that will go through a CI/CD process will need dynamic SQL to incorporate changing volume paths based on catalog or schema names based on environments.  \n",
    "\n",
    "For this first bronze table, we'll use Autoloader's SQL function `read_files`. When data is in the JSON format, Autoloader can automatically infer the schema.  However this requires each JSON file to have the exact same schema, and due to the nature of FHIR data, no two JSONs are guaranteed to have the same schema.  This implies that normal methods for parsing JSON files are not possible with FHIR. \n",
    "\n",
    "There is a new data type called VARIANT, and it is able to parse every JSON file completely independently, including inside a stream.  To use the VARIANT data type we must either use a Databricks SQL Warehouse or Databricks Runtime 15.3LTS+.  VARIANT is in public preview. It requires Delta Lake file format, which is the default managed table file format in Databricks.  In order to use VARIANT we'll need to set the Lakeflow Pipeline channel to preview.\n",
    "\n",
    "To transform a JSON string to VARIANT we use the `parse_json` Spark SQL function.  One thing to note about parse_json is that it must be given well-formed JSON strings.  If the JSON string is malformed then `parse_json` will return an error.  To simply return a NULL instead of an error we can use `try_parse_json` instead.  \n",
    "\n",
    "In order to have a record of everything that we've recieved from our FHIR source, in this fhir_bronze table we'll ingest the bundles as text. Then in the next bronze table we'll use try_parse_json.\n",
    "\n",
    "Note that Synthea produces some malformed FHIR JSONs on purpose to simulate what you'd expect in a real world setting where your FHIR data is coming from multiple sources.  This is where brokers like Redox come in-- Redox ensures that each transaction you receive is always well formed and follows the same general best practices as every Redox bundle produced.  This ensures better uniformity downstream.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b7e8b43-cfff-46fc-929f-f5d8637d8f8d",
     "showTitle": true,
     "tableResultSettingsMap": {
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765642476813}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": "Dynamically Create the FHIR Bronze Streaming Table Statement"
    }
   },
   "outputs": [],
   "source": [
    "DECLARE OR REPLACE VARIABLE create_streaming_bronze STRING;\n",
    "\n",
    "SET VARIABLE create_streaming_bronze = \"\n",
    "CREATE OR REFRESH STREAMING TABLE fhir_bronze (\n",
    "  file_metadata STRUCT<\n",
    "    file_path: STRING,\n",
    "    file_name: STRING,\n",
    "    file_size: BIGINT,\n",
    "    file_block_start: BIGINT,\n",
    "    file_block_length: BIGINT,\n",
    "    file_modification_time: TIMESTAMP> NOT NULL COMMENT 'Original meta date of the file ingested from the volume.'\n",
    ", ingest_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP() COMMENT 'The date timestamp the file was ingested.'\n",
    ", bundle_uuid STRING NOT NULL COMMENT 'Unique identifier for the FHIR bundle.'\n",
    ", value STRING COMMENT 'Original JSON record ingested from the volume as a full text string value.'\n",
    ")\n",
    "COMMENT 'Ingest FHIR JSON records as Full Text STRING'\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true',\n",
    "  'delta.enableDeletionVectors' = 'true',\n",
    "  'delta.enableRowTracking' = 'true',\n",
    "  'quality' = 'bronze'\n",
    ")\n",
    "AS SELECT\n",
    "  _metadata as file_metadata\n",
    ", uuid() as bundle_uuid\n",
    ", * \n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/nebraska_health_demo/\" || schema_use || \"/landing/'\n",
    ", format => 'text'\n",
    ", wholeText => true\n",
    ")\n",
    "\"\n",
    ";\n",
    "\n",
    "-- SELECT create_streaming_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddfa7378-24ac-47a8-9fd0-6be638d5a0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the above SQL note the use of the DDL to set the column types, defaults and comments.  We also apply a table comment, and table properties.  The table properties here are critical for the efficient processing of our streaming data to ensure we always apply incremental updates, including for later SQL that will require `PIVOT`s or materialized views.  Setting your table properties correctly vastly improves performance and lowers compute costs.  \n",
    "\n",
    "- **Enabling Change Data Feed** allows the stream or a data engineer to be able to read exactly from a particular microbatch.  \n",
    "- **Deletion vectors**, now enabled by default on new managed Delta tables performs a soft delete using meta data on initial processing, and then later a background process physcially deletes the records from the delta tables, vastly increasing performance during active loads.  \n",
    "- **Row Tracking** adds additional metadata such that streams or materialized views can be sure which data has been recently updated, inserted or deleted, resulting in incremental loads or streams inside of full refreshes.  This is required to use `PIVOT` in streaming SQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517aa813-001f-4ea0-aff6-4923106742f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Excecute the FHIR Bronze Statement"
    }
   },
   "outputs": [],
   "source": [
    "EXECUTE IMMEDIATE create_streaming_bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3741825c-69e6-4014-878e-873983325c9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query the fhir_bronze table"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "SELECT \n",
    "  file_metadata\n",
    ", ingest_time\n",
    ", bundle_uuid\n",
    ", substr(value FROM 0 FOR 10000) as value -- Show just the first 10,000 characters of the text \n",
    "FROM \n",
    "  fhir_bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412521b1-1f1f-4533-9739-e1856f22e6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now create the second bronze table in which we turn the 'value' column from STRING type to VARIANT type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb20b2fd-6b55-4bc2-b11d-a583dac1cfd5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop fhir_bronze_variant"
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS fhir_bronze_variant;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ba4e4c-d535-4b1e-9402-0e6705a34668",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create or refresh fhir_bronze_variant"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE fhir_bronze_variant (\n",
    "  bundle_uuid STRING NOT NULL COMMENT 'Unique identifier for the FHIR bundle.'\n",
    ", ingest_time TIMESTAMP NOT NULL COMMENT 'The date timestamp the file was ingested.'\n",
    ", file_metadata STRUCT<\n",
    "    file_path: STRING,\n",
    "    file_name: STRING,\n",
    "    file_size: BIGINT,\n",
    "    file_block_start: BIGINT,\n",
    "    file_block_length: BIGINT,\n",
    "    file_modification_time: TIMESTAMP> NOT NULL COMMENT 'Original meta date of the file ingested from the volume.'\n",
    ", fhir VARIANT COMMENT 'Original JSON record fully parsed as a variant data type.'\n",
    ")\n",
    "COMMENT 'Evaluate FHIR JSON records as VARIANT'\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true'\n",
    ", 'delta.enableDeletionVectors' = 'true' \n",
    ", 'delta.enableRowTracking' = 'true'\n",
    ", 'quality' = 'bronze'\n",
    ", 'pipelines.channel' = 'PREVIEW' -- 20251213: Is this still needed?\n",
    ", 'delta.feature.variantType-preview' = 'supported'\n",
    ")\n",
    "AS SELECT\n",
    "  bundle_uuid\n",
    ", ingest_time\n",
    ", file_metadata\n",
    ", try_parse_json(value) as fhir \n",
    "FROM STREAM fhir_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb6bd0cb-3be8-45aa-800b-b11d478ca666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Results too large is common sometimes with the extremely large C-CDA style FHIR bundles that Synthea generates.  In the real world you'll have a mix of FHIR bundles that are based on Prior Authorization Requests, single Observations, ADTs, etc that will be of smaller size and the results can be displayed in the notebook code chunk. You can switch over to the SQL Editor to look at the new 'fhir' column if the results are too large to show in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9711801-1acd-4e0c-86ef-1608727a2c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that we cannot use SQL to query the information in a VARIANT column without knowing anything ahead of time about its original schema.  For traditional from_json methods to work, including inferring the JSON schema, each file should have the same consistent schema.  \n",
    "\n",
    "However with FHIR, every bundle received could have any number of resource arrays and they could be in any order.  For example, there is no guarantee that the first resource in the entry array is the Patient array.  Synthea, used here, and Redox (a Databricks partner) have very different standard practices for what is used or not used in the bundle, yet both are valid FHIR bundles.  \n",
    "\n",
    "For example, Redox always includes a bundle level meta object, and the first resource in their entry array is typically also a meta resource.  Synthea does not include the meta bundle object, and typically has the Patient resource first in the entry array for C-CDA converted bundles (though this need need be the case for all types of bundles).  Additonally Synthea occasionally produces malformed JSON files, whereas Redox only sends perfectly formed JSON files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bd9eefc-de6a-4c9f-99ac-8e683ac820a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How do you use SQL to get particular values out of a VARIANT column?**\n",
    "\n",
    "- Use the `<variant_colname>:<path>::<datatype>` method where a single colon `:` indicates an entry path for the data selected, and the double colon `::` is a shorthand for casting the value returned as the desired datatype.  \n",
    "- If we prefer to be verbose, we can always use the `CAST(expr as datatype)` function outright.  \n",
    "- And `variant_get` combines the JSON paths that many are familiar with, including the `$.` notation, with a `CAST` where we provide the desired datatype as a string value. \n",
    "\n",
    "Note that if we don't cast the selected VARIANT data to a different datatype, then it will remain as a VARIANT column.  This is desirable to continue to use for structs until a final schema may be applied, however VARIANT column types can not be used as group by clauses for aggregation functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7296e0e5-344c-4bb5-b4fe-0eae74be66b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's Make a Map!"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "select \n  bundle_uuid\n  ,CAST(fhir:entry[0].resource.address[0].extension[0].extension[0].valueDecimal as double) as latitude\n  ,fhir:entry[0].resource.address[0].extension[0].extension[1].valueDecimal::double as longitude\nfrom \n  fhir_bronze_variant\nwhere \n  fhir is not null\n  and variant_get(fhir, '$.entry[0].resource.resourceType', 'STRING') = 'Patient';",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "MAP"
         },
         {
          "key": "options",
          "value": {
           "backgroundColor": "#356AFF",
           "borderColor": "#356AFF",
           "bounds": {
            "_northEast": {
             "lat": 44.6145720297899,
             "lng": -92.65546069875909
            },
            "_southWest": {
             "lat": 38.56604871137493,
             "lng": -105.79510913625909
            }
           },
           "clusterMarkers": false,
           "customizeMarkers": false,
           "foregroundColor": "#ffffff",
           "groups": {},
           "iconFont": "circle",
           "iconShape": "marker",
           "latColName": "latitude",
           "lonColName": "longitude",
           "mapTileUrl": "//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png",
           "popup": {
            "enabled": true,
            "template": ""
           },
           "showPlotlyControls": true,
           "tooltip": {
            "enabled": false,
            "template": ""
           }
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "218ceb0d-6363-4f15-9e76-f2c8ba44ae97",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 32.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%skip\n",
    "select \n",
    "  bundle_uuid\n",
    ", CAST(fhir:entry[0].resource.address[0].extension[0].extension[0].valueDecimal as double) as latitude\n",
    ", fhir:entry[0].resource.address[0].extension[0].extension[1].valueDecimal::double as longitude\n",
    "from \n",
    "  fhir_bronze_variant\n",
    "where \n",
    "  fhir is not null\n",
    "  and variant_get(fhir, '$.entry[0].resource.resourceType', 'STRING') = 'Patient';"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Streaming Bronze Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
