{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "474124e1-f6b1-4b1f-b910-6148c23eeeed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using TaskValues to Dynamically Pass Information Between Tasks in a Databricks Workflow \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c2c8a8-94c1-457e-abfd-1974e8e753ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use the Catalog and Schema and Verify Its Set"
    }
   },
   "outputs": [],
   "source": [
    "schema_use = 'nebraska_medicine_demo'\n",
    "spark.sql(f\"USE mcutini.{schema_use}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f7faa32-0bfe-46dd-bb58-be712ea29562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Decide which ResourceTypes you want to turn into silver tables. Note that unlike other methods, this ensures we're not wasting compute on Resources we've never seen before, nor run the risk of missing a new Resource should it enter our system for the first time in a following refresh.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632acfb4-c1d0-45f8-8760-953852bbe38c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use SQL to Identify the Resource Types Available"
    }
   },
   "outputs": [],
   "source": [
    "resource_types = spark.sql(\"SELECT DISTINCT resourceType FROM fhir_bronze_resource_schemas\").collect()\n",
    "resource_types = [row.resourceType for row in resource_types]\n",
    "resource_types.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8862e8e0-70ed-4cdf-bfa6-ac91b24f6f27",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "This final step is the only reason its a Python Notebook and Not SQL"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(\"resource_types\", resource_types)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05 - Task Values",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
